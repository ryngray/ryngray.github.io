We present an algorithmic framework for creating Readable Tree Layouts. Even though the ``tree layout problem" is an old, well-known, and some-would-say solved problem, we define the ``readable tree layout problem" and show that there is more work to be done in this domain. 
Such layouts can be used to drive visualization systems for large graphs, as shown in this short video.
What is a “readable tree layout” then? Well, most real-world graphs have labeled nodes and edges that capture similarity/dissimilarity values (rather than just connectivity). For example, one of the graphs we work with is the google research topics graphs: nodes are research topics and edges connect similar topics. Looking at a plain node-link representation of this graph without the labels and ignoring the similarity between pairs of topics can give us a general idea of its global structure, but it misses key insights, such as which research topics are closely related, what groups of topics there are, and so on.  We define 2 hard constraints and 2 properties that should be optimized for readable tree layouts.
The first requirement is that node labels should be readable and should not overlap.  As seen in this image, it is impossible to read many of the labels on the left, due to the overlapping nodes.  On the other hand, our algorithmic framework produces the layout shown on the right, which is more readable as none of the words overlap.
The second requirement is that layouts should have no edge crossings.  As shown in the literature, edge crossings reduce readability of graphs and can be completely avoided in trees. Determining path length is harder in the example on the left than in the output from our algorithm on the right.
The first property to optimize is “desired edge lengths”. In many real-world datasets edge lengths are meaningful: for example, in “the tree of life” the length of an edge corresponds to the evolutionary time separating two species. Even in unweighted graphs, the desired edge length should be uniform as significant edge length deviations can create false impressions of clusters, where there are none.
The second property to optimize is “compactness”.  If we ignore compactness, we could take any crossing-free tree drawing and scale it DOWN until labels no longer overlap.  However, this leaves us with a lot of “wasted” white space in our drawing and requires a lot of zooming to read any labels.  In the example here, the right shows the output from one of our methods, while the image on the left is obtained from another algorithm that ignores compactness. Our compact layout allows us to see more of the underlying data, without needing to zoom out (which would make the labels more difficult to read).
In addition to the two hard constraints (no overlaps, no crossings) and the two soft constraints (edge lengths and compactness) we also care about scalability. With this in mind, we develop a parallel version of our algorithm: not all graphs and trees are small, and practical tree layout algorithms should be able to handle large inputs. We show that the parallel algorithms can handle inputs with hundreds of  thousands of  nodes in seconds.  For this parallel algorithm, we use OpenMP
To summarize, we have two hard constraints: no label overlaps and no edge crossings and two optimization goals: realizing desired edge lengths and compactness, while also providing scalability via parallelization.
With these motivations, we note that several of them can lead to contradictory results.  For example, suppose we have a graph where a single node is connected to many others with the same desired edge length.  If we keep the edge lengths, this will lead to a layout that is not compact.  If we put all of the nodes together in a very compact way, this will lead to not all of the edges achieving their desired lengths.  For this reason, we give two variations of our algorithm, one that prioritizes desired edge lengths and one that prioritizes compactness.
Here is an overview of our algorithms.  We have the Readable Tree layout (RT), which needs just a single processor; and Parallel Readable Tree layout (PRT) which can draw large graphs with hundreds of thousand nodes, using multiple processors. Each of these algorithms has two options, one focuses on edge lengths  (L), other focuses on compactness (C). Note that these two optimization goals (edge length realization and compactness) are incompatible, which motivates the two variants.
Our readable tree layout algorithm follows these basic steps: initial layout, iterative improvements, and a final output check. They DO differ in how these steps are performed and what layout aspect they optimize.  
Before we can get to the layout algorithm, we need to spend some time preprocessing the data to create a multi-layer representation of the input graph.  Many graphs already come with node weights and edge weights, but if the graph is unweighted we assign weights based on standard measures.  A multi-level Steiner tree algorithm extracts a suitable hierarchy of trees (with the input graph on the deepest level and progressively smaller representations on the levels above).  This hierarchy is needed in the final map visualization (for semantic zooming), while the layout algorithm computes positions for the entire graph (the deepest level). Desired edge lengths are also required.  These can be given by the edge weights, if only the layout algorithm is used, or by the level of tree that the edge first appears on, if the map-like visualization is used.
To help the options with desired edge lengths and compactness, we provide two initial layout algorithms.  The desired edge length one keeps the edge lengths exactly, but does this at the expense of compactness and the compactness keeps the layout relatively compact, but does not take the desired edge lengths into consideration.  Both of these algorithms ensure that we have no edge crossings in the initial step.  
Given an initial layout optimizing edge lengths or compactness, a force directed algorithm is employed to improve the soft constraints, while maintaining the hard ones. The forces involved are: a force to preserve edge lengths, a force to remove overlaps, a distribution force (which is a node-node force to keep the nodes from getting too close), and a node-edge force that tries to keep the labels from overlapping the edges.  We ensure that no edge crossings are introduced, by not performing any move that would create a crossing.
Here is a comparison of our layouts, in the box, RT_L, RT_C, PRT_L and PRT_C compared with two already existing algorithms.  CIR is a circular layout algorithm from the YFiles system and SFDP+prism is the Scalable Force-Directed Algorithm from the GRAPHVIZ library, followed by an overlap removal phase. Algorithms such as CIR produce no edge crossings but distort edge lengths and might suggest a graph structure that is not necessarily there.  SFDP+prism produces compact layouts that represent the underlying structure well, but  they do have many edge crossings.  We provide a more careful analysis next.
We have three evaluation criteria: desired edge length realization, compactness, and runtime.
In the paper, we include the relevant metric and exact numbers for our test graphs, but here I will briefly discuss a more qualitative evaluation.  In the image below we see the same graph laid out by the RT_L and RT_C algorithms.  Edges in these layouts are darker if the length of that edge is far from the desired edge length (redder if compressed, bluer if stretched).  The RT_C algorithm has a worse average edge length deviation, while the RT_L does its job with very low edge length deviation. 
Here we can see a closer view of the graph. Note that the RT_C algorithm has smaller errors that affect most of the edges, while the RT_L algorithm has only a few edges that do not match up well, but these may be very off. 
This table gives the numeric values of the differences in edge lengths; the best score for each graph is bold. As expected, RT_L, our edge length algorithm performs better on the edge lengths up until it cannot draw the graphs anymore.  At this point, our parallel algorithm, PRT_L and SFDP+Prism are comparable.  Although, as we will note many times, SFDP+Prism includes edge crossings.
Next, we look at the compactness.  While these two visualizations may look like they take the same space, the CIR drawing has been scaled by a factor of 40000 to fit on the same scale.  This means that to read the nodes on CIR, we have to scale down a lot and will end up with mostly white space.  For comparison, our algorithm gives the same number of nodes in a much more compact space.
This table gives the numeric values for the compactness measurement. This is given by taking the sum of the areas of all the labels and dividing it by the total area used in the layout, resulting in a value between 0 and 1, with higher scores corresponding to better area utilization. The best score for each graph is bold.  This figure shows that on most of the input graphs, our compactness algorithm, RT_C or its parallel version PRT_C outperforms the others. SFDP+Prism produces compact layouts for large inputs but with they have many edge crossings.
Finally, we have these graphs which show how the PRT algorithm scales with respect to the number of cores and the number of edges.  Note that in terms of the number of nodes, the PRT algorithm scales linearly.  This allows for drawing much larger graphs than the RT_L or RT_C algorithm.
Here we give the runtime of each of these algorithms.  Our parallel algorithm PRT outperforms all except SFDP+Prism which can, and very often does, produce edge crossings. 
Now we will show a quick demo of our visualization system.  Note that in the beginning, we set up a hierarchy of trees.  As we scroll down into the graph, we see more and more nodes come into view as we reach deeper levels of trees.  We use GMAP as a base for the clustering and country metaphors.  You can see here that our labels do not overlap throughout our scrolling.  Edge lengths are based on the level their source and target show up in.
We will discuss the limitations of this work briefly here.  First, We believe that compact, crossing-free, overlap-free layouts are important for readability, but this remains to be validated with human subject studies. Next, we only compare against two prior algorithms.  This was because we could not find more than two that did even some of our requirements and no algorithms that tried to do them all. Next, we know that force directed algorithms give a large parameter space to explore.  While we found good parameters for our problems, much of this parameter space remains unexplored.  Finally, many other overlap removal tools exist.  It would be interesting to see them be modified to keep the topology of the graph, for example, maintain no edge crossings, while removing the label overlaps.
If you would like to learn more, the link is there.  On that page you can see more examples with a map-like visualization of these layouts.  Also on the site are links to the paper and code.
